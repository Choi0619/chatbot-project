import os
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
from transformers import pipeline

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ëª¨ë¸ê³¼ ê°ì • ë¶„ì„ ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=api_key)
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# ëŒ€í™” ë§¥ë½ ìœ ì§€ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì„¤ì •
memory = ConversationBufferMemory()

# Streamlit UI ì„¤ì •
st.title("ğŸŒ¸ ë§ˆìŒ ì‰¼í„° ìƒë‹´ ì±—ë´‡ ğŸŒ¸")
st.write("ì•ˆë…•í•˜ì„¸ìš”! ì–¸ì œë“ ì§€ ë§ˆìŒì˜ ë¶€ë‹´ì„ ë‚˜ëˆ ë³´ì„¸ìš”. ë”°ëœ»í•˜ê²Œ ê·€ ê¸°ìš¸ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# UI ìŠ¤íƒ€ì¼ë§ CSS ì ìš©
st.markdown("""
    <style>
    .user-message { background-color: #E0F7FA; padding: 10px; border-radius: 10px; margin: 10px 0;}
    .assistant-message { background-color: #FFF3E0; padding: 10px; border-radius: 10px; margin: 10px 0;}
    .container { max-width: 650px; margin: auto;}
    </style>
""", unsafe_allow_html=True)

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    role, content = message["role"], message["content"]
    style_class = "user-message" if role == "user" else "assistant-message"
    st.markdown(f"<div class='{style_class}'>{content}</div>", unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì €ì—ê²Œ ë³¸ì¸ì˜ ë§ˆìŒì„ í„¸ì–´ë†“ì•„ë³´ì„¸ìš”..."):
    # ê°ì • ë¶„ì„ ìˆ˜í–‰
    sentiment_result = sentiment_analyzer(prompt)[0]
    sentiment = sentiment_result['label']

    # ê°ì • ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ í†¤ ì„¤ì •
    if sentiment in ["1 star", "2 stars"]:  # ë¶€ì •ì ì¸ ê°ì •
        tone = "ìœ„ë¡œì™€ ê³µê°ì„ ë“œë¦¬ëŠ” ì¡´ëŒ“ë§ë¡œ"
    elif sentiment in ["4 stars", "5 stars"]:  # ê¸ì •ì ì¸ ê°ì •
        tone = "ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ì¡´ëŒ“ë§ë¡œ"
    else:  # ì¤‘ë¦½ì ì¸ ê°ì •
        tone = "í¸ì•ˆí•˜ê³  ê³µê°ì ì¸ ì¡´ëŒ“ë§ë¡œ"

    # ì‚¬ìš©ì ì…ë ¥ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ëŒ€í™” ë§¥ë½ì„ í¬í•¨í•˜ì—¬ ëŒ€í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
    conversation_history = memory.load_memory_variables({}).get("history", "")
    question_template = PromptTemplate(
        input_variables=["tone", "conversation_history", "user_input"],
        template="{tone} ë‹µë³€í•´ ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™”: {conversation_history} ì‚¬ìš©ì ì§ˆë¬¸: {user_input}"
    )
    formatted_prompt = question_template.format(
        tone=tone, conversation_history=conversation_history, user_input=prompt
    )

    # GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    answer = llm([HumanMessage(content=formatted_prompt)]).content
    with st.chat_message("assistant"):
        st.markdown(answer)
    st.session_state.messages.append({"role": "assistant", "content": answer})

    # ëŒ€í™” ë§¥ë½ ì €ì¥
    memory.save_context({"input": prompt}, {"output": answer})
